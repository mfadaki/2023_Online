# Problem: To set the temperature of the shower to a range by decreasing/increasing the

import tensorflow as tf
from rl.memory import SequentialMemory
from rl.policy import BoltzmannQPolicy
from rl.agents import DQNAgent
# from tensorflow.keras.optimizers import Adam
from tf.keras.layers import Dense, Flatten
from tf.keras.models import Sequential
import random
import numpy as np
from gym.spaces import Discrete, Box
from gym import Env


""" !pip3 install tensorflow
!pip3 install gym
!pip3 install keras
!pip3 install keras-rl2  # did not work
!pip3 install keras-rl2 """

# 1. Create an Environment


class ShowerEnv(Env):
    def __init__(self):
        # Actions we can take, down, stay, up
        # Discrete creates a discreate space
        self.action_space = Discrete(3)

        # Temperature array
        # Box creates a continuous space
        self.observation_space = Box(low=np.array([0]), high=np.array([100]))

        # Set start temp (state)
        self.state = 38 + random.randint(-3, 3)

        # Set shower length
        self.shower_length = 60

    def step(self, action):
        # Apply action
        # 0 -1 = -1 temperature
        # 1 -1 = 0
        # 2 -1 = 1 temperature
        self.state += action - 1
        # Reduce shower length by 1 second
        self.shower_length -= 1

        # Calculate reward
        if self.state >= 37 and self.state <= 39:
            reward = 1
        else:
            reward = -1

        # Check if shower is done
        if self.shower_length <= 0:
            done = True
        else:
            done = False

        # Apply temperature noise
        # self.state += random.randint(-1,1)

        # Set placeholder for info
        info = {}

        # Return step information
        return self.state, reward, done, info

    def render(self):
        # Implement visualisation of the outcome
        pass

    def reset(self):
        # Reset shower temperature
        # The system gets back to the Initial (start) state
        self.state = 38 + random.randint(-3, 3)

        # Reset shower time
        self.shower_length = 60
        return self.state


# Instantiate the class
env = ShowerEnv()

# Check a sample from the observation space
env.observation_space.sample()

# Without having a model, just make 10 episodes and see the score of each episode
episodes = 10
for episode in range(1, episodes+1):
    state = env.reset()
    done = False
    score = 0

    while not done:
        # env.render()
        action = env.action_space.sample()
        n_state, reward, done, info = env.step(action)
        score += reward
    print('Episode:{} Score:{}'.format(episode, score))

# 2. Create a Deep Learning Model with Keras

# Define all states and actions for the model
states = env.observation_space.shape
actions = env.action_space.n

print(actions)

# Building the NN model


def build_model(states, actions):
    model = Sequential()
    model.add(Dense(24, activation='relu', input_shape=states))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model

# To delete the model if necessary
# del model


model = build_model(states, actions)

model.summary()

# 3. Build Agent with Keras-RL


def build_agent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy,
                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn


dqn = build_agent(model, actions)
dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)

# Testing the results
scores = dqn.test(env, nb_episodes=100, visualize=False)
print(np.mean(scores.history['episode_reward']))

_ = dqn.test(env, nb_episodes=5, visualize=False)

# 4. Reloading Agent from Memory
# Saving the model
dqn.save_weights('dqn_weights.h5f', overwrite=True)

# To reload the model
del model
del dqn
del env

dqn.load_weights('dqn_weights.h5f')

_ = dqn.test(env, nb_episodes=5, visualize=False)
